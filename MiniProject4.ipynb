{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fd35a65-9280-472c-8f0a-62ea09c95b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking images from camera\n",
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def testVideoCam():\n",
    "    while True:\n",
    "        has_frame, frame = cap.read()\n",
    "        if not has_frame:\n",
    "            print('No frames are captured')\n",
    "            break\n",
    "        #frame = cv2.resize(frame,(160,160))\n",
    "        cv2.imshow('testFrame',frame)\n",
    "        key = cv2.waitKey(3)\n",
    "        if key == 27:\n",
    "            print('Pressed Esc')\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return\n",
    "\n",
    "#testVideoCam()\n",
    "\n",
    "#capture image for training\n",
    "for i in range(10):\n",
    "    has_frame, frame = cap.read()\n",
    "    if not has_frame:\n",
    "        print('No frames are captured')\n",
    "    #frame = cv2.resize(frame,(160,160))\n",
    "    \n",
    "cv2.imshow('testFrame',frame)\n",
    "cv2.waitKey(0)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca30f7bc-c258-4017-88ab-13a5dee1a442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the trainImage\n",
    "filepath = \"C:\\\\FaceRecognitionFiles\\\\\"\n",
    "cv2.imwrite(filepath+'trainImage.jpg',frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a095d1e-49bf-42a9-b030-db2e429582b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect face from saved trainImage\n",
    "from PIL import Image\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "detector = MTCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "449dec85-8085-46f7-99ba-c25ad0af231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the image in frame variable using opencv\n",
    "frame = cv2.imread(\"C:\\\\FaceRecognitionFiles\\\\trainImage.jpg\")\n",
    "\n",
    "# Loading haar cascade classifier in classifier variable using opencv\n",
    "classifier = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Pre-processing the image, converting it to greyscale\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detecting faces from image using classifier (scalefactor[how much image is reduced at each scale], minimum no of neighbours[number of neighbours each candidate should have to retain it])\n",
    "faces = classifier.detectMultiScale(gray, 1.1, 6)\n",
    "#faces is a 2d array containing only xywh of all the people detected in the image\n",
    "#frame = cv2.imread(\"C:\\\\FaceRecognitionFiles\\\\trainImage.jpg\")\n",
    "#trainface_metadata = detector.detect_faces(frame) #extracting faces from trainImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69cb4848-bccc-4dc8-80c1-abdcd3fd2b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of faces detected =  1\n"
     ]
    }
   ],
   "source": [
    "print('number of faces detected = ', len(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb3ca263-ab5e-4dd5-ad18-17cac3d09cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224 162 233 233]\n"
     ]
    }
   ],
   "source": [
    "print(faces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8752b1ba-2757-4801-a484-e8b93ea1167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detecting face coordinates\n",
    "#x = int(trainface_metadata[0]['box'][0])\n",
    "#y = int(trainface_metadata[0]['box'][1])\n",
    "#w = int(trainface_metadata[0]['box'][2])\n",
    "#h = int(trainface_metadata[0]['box'][3])\n",
    "x = faces[0][0]\n",
    "y = faces[0][1]\n",
    "w = faces[0][2]\n",
    "h = faces[0][3]\n",
    "detected_face = cv2.rectangle(frame, (x,y),(x+w,y+h), color=255, thickness=3) #see the detected face outlines\n",
    "cv2.imshow('Detected face with rectangle',detected_face)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af575749-afb0-4488-888b-3fad2b153386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cropping the image to required size 160,160\n",
    "face = frame[y:y+h, x:x+w]\n",
    "face = Image.fromarray(face)\n",
    "face = face.resize((160,160))\n",
    "face = np.asarray(face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f3dbcfb-a9f4-4da9-b72b-fd1b68f7853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display cropped image\n",
    "cv2.imshow('Cropped image', face)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b95cfbc-f202-46b8-ada0-bd4f51da4466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create face embedding\n",
    "import tensorflow as tf\n",
    "from keras_facenet import FaceNet\n",
    "embedder = FaceNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f40b7c6-6c03-4c1e-9bd7-34fa672a7002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#face embedding generator function\n",
    "def get_embedding_facenet(face_pixels):\n",
    "    face_pixels = face_pixels.astype('float32')\n",
    "    mean, std = face_pixels.mean(), face_pixels.std()\n",
    "    face_pixels = (face_pixels - mean)/std\n",
    "    samples = np.expand_dims(face_pixels, axis=0)\n",
    "    embedding = embedder.model.predict(samples)\n",
    "    return embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "114cf06a-cb11-4d80-8732-e97a9e78f4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "trainface_embedding = get_embedding_facenet(face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0089d56-a08e-4842-ba01-efce6f73235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04152646  0.02082708  0.01599292  0.03894857  0.05321268  0.05876885\n",
      "  0.00184753 -0.00677577 -0.01167484 -0.06622963 -0.05329586 -0.01011575\n",
      "  0.00689343 -0.00511622  0.02425072 -0.03728476 -0.01084804  0.05191188\n",
      "  0.03832323  0.02062868 -0.01962681  0.01932536  0.06187895 -0.01828597\n",
      "  0.06980556  0.03568329  0.10337234 -0.05111944  0.06456415 -0.05201211\n",
      " -0.01805717  0.02797248 -0.07328796  0.06159768  0.06526904  0.03633595\n",
      "  0.04137305  0.00219036 -0.05876808  0.01826699  0.08816267 -0.05229949\n",
      " -0.04220127 -0.06203759 -0.08094054 -0.05785367 -0.0295382   0.05436934\n",
      " -0.09175251 -0.0127039   0.0009308  -0.04197019  0.00408793  0.07499621\n",
      "  0.00108911 -0.02617867  0.03994458  0.03353146  0.05709821  0.02073997\n",
      "  0.03979392 -0.00719269  0.04108038  0.00331393  0.01807653 -0.00948615\n",
      "  0.05806514 -0.02403394  0.03069177 -0.06462449 -0.01127922 -0.0376946\n",
      " -0.00197306  0.01579353  0.04970215 -0.01917705 -0.01071534 -0.01183518\n",
      " -0.02006381 -0.04264342  0.06740677 -0.01144532  0.0232173  -0.03031314\n",
      " -0.04311703  0.04946629  0.02608789 -0.04560184 -0.0137604   0.03744063\n",
      "  0.03417176  0.05812744 -0.03301617 -0.02765481  0.01641804 -0.02706548\n",
      " -0.03335838 -0.02925308 -0.16594473  0.00436961 -0.00213319 -0.05004521\n",
      " -0.00250841  0.01199963 -0.06469142 -0.00236015  0.06659923 -0.00527416\n",
      " -0.07042954  0.00429608 -0.01378986 -0.02302735 -0.02041703  0.026396\n",
      "  0.00341588 -0.02981879 -0.03293259  0.03968203 -0.02881126  0.01547218\n",
      " -0.00634713  0.04524145  0.04919562 -0.04529084 -0.07471217 -0.0416994\n",
      " -0.00662262 -0.03721547 -0.04842179  0.00534027  0.00643028 -0.01166928\n",
      " -0.04833029 -0.04579623 -0.06582901  0.04444531  0.01451063 -0.01920854\n",
      " -0.04356019  0.01600985  0.03163217 -0.0353268   0.01802104  0.0470129\n",
      " -0.00867047  0.00864793  0.0088258   0.00472247  0.03744045 -0.10957632\n",
      "  0.05455538  0.03198943 -0.07457046 -0.0172893   0.02005204 -0.02733047\n",
      " -0.0175878   0.05740751 -0.03499133  0.07349663 -0.0161766   0.06522114\n",
      "  0.01130263  0.00420932 -0.0166662   0.07647588 -0.11078364 -0.04918719\n",
      " -0.03544846  0.04302682 -0.02772005 -0.00364185  0.03959775 -0.01481036\n",
      "  0.09273383 -0.04973191 -0.00431488 -0.05973012 -0.09110252  0.05299241\n",
      " -0.02034047  0.10695091 -0.02679709 -0.02108856 -0.01700516 -0.01294479\n",
      "  0.04752156 -0.02654734  0.04428737 -0.01052453  0.00781864  0.06006337\n",
      "  0.04266454 -0.03066199 -0.03901318 -0.01951499 -0.07109365  0.01320398\n",
      "  0.0142439   0.00160875  0.00576619 -0.0446556  -0.01207707  0.09889252\n",
      "  0.01737472 -0.02309409 -0.02055099  0.040022    0.00031102  0.04229362\n",
      " -0.01950855 -0.07635977 -0.07604289  0.06605448 -0.0472946  -0.00619634\n",
      "  0.01814304  0.02798288 -0.0464162   0.06207133 -0.05056744 -0.05130007\n",
      " -0.04112171 -0.04743385 -0.01734789  0.12947586  0.01207945  0.00487115\n",
      " -0.03185368 -0.01692289  0.00901581  0.06109482  0.02481001 -0.04621629\n",
      " -0.00391822  0.02134111 -0.03903563 -0.01285585 -0.00534502  0.00136235\n",
      " -0.1060719  -0.04225188 -0.03765609  0.03100021  0.05012421  0.03507087\n",
      "  0.00919395 -0.08009115  0.00267409  0.00558675  0.0421406  -0.00136394\n",
      " -0.01630355  0.00121577  0.01164387 -0.02212887 -0.00160548  0.03839693\n",
      " -0.05281135  0.07260628  0.03984763  0.03258773 -0.01535347 -0.06865627\n",
      " -0.09058242 -0.02705713  0.04389878 -0.00409901 -0.02583667  0.01873126\n",
      "  0.01615988 -0.0019175   0.001509   -0.0506161   0.0270461   0.04068167\n",
      " -0.04155148 -0.05366372  0.06370839  0.0659547  -0.02053855  0.03365177\n",
      " -0.11773613  0.01837976 -0.11166018 -0.03916299 -0.03795931 -0.01827769\n",
      "  0.05539133  0.0320805  -0.02392884 -0.06064473  0.02979131 -0.03057456\n",
      "  0.01623491 -0.01418081  0.00345421  0.05232214 -0.07509143 -0.02113509\n",
      "  0.00857369 -0.07351026  0.0199056   0.0806001  -0.01480354 -0.09664468\n",
      " -0.01540344  0.03160869 -0.03910119 -0.03169538 -0.01253114  0.01562912\n",
      "  0.01716473  0.07811286 -0.00094483  0.0752821   0.01588819 -0.03404188\n",
      "  0.03269909  0.04121663  0.09692632 -0.02317868  0.01094663  0.07067324\n",
      "  0.02249608  0.00183382  0.05247277  0.0097292  -0.02797357  0.00699179\n",
      "  0.00683165 -0.0835152   0.03434736 -0.04692029  0.01107906 -0.00102389\n",
      "  0.08733146 -0.0039942   0.02687761  0.02080245 -0.04221173 -0.0498107\n",
      " -0.01878756 -0.06079223  0.03538572 -0.02561046 -0.0266384   0.10956315\n",
      " -0.05634358  0.02355745 -0.04391862  0.03960744 -0.00934743  0.00871768\n",
      " -0.09798694  0.03980023  0.06255687  0.01640228 -0.00116848  0.0677043\n",
      "  0.08303327 -0.01055221  0.00431962 -0.00232669 -0.03826991  0.03422674\n",
      " -0.00935757  0.02148713  0.0211216   0.01298413 -0.01443092 -0.01009015\n",
      " -0.00718168 -0.00929444 -0.01355339  0.0344943   0.00853764 -0.02863924\n",
      "  0.01529457  0.00173553 -0.02051596  0.04520814  0.03704937 -0.04659983\n",
      "  0.04393256 -0.0758717  -0.08796676  0.03388236  0.04137158 -0.01093092\n",
      " -0.00564581  0.05167817 -0.0485085  -0.00732698 -0.04562892 -0.03638392\n",
      "  0.01475594  0.07315281 -0.01987299  0.03519463 -0.09793638  0.0478894\n",
      " -0.04655218  0.00534954 -0.01414924 -0.01344535 -0.00834376 -0.0549334\n",
      "  0.09800249  0.09438532 -0.01354253 -0.00783623  0.00884796  0.01962382\n",
      " -0.02518023 -0.10328913 -0.04785607 -0.05924374  0.01645558 -0.10259283\n",
      " -0.04337988  0.03256208 -0.04091042  0.04518734 -0.0190951  -0.01077401\n",
      " -0.00186112 -0.05094545  0.00332543 -0.0441042   0.03006249  0.03936443\n",
      "  0.01994058 -0.02821905 -0.03338446 -0.01235453 -0.00457478 -0.07308193\n",
      "  0.02483598 -0.04985913 -0.04302397  0.01055268 -0.05237198 -0.00370697\n",
      " -0.0411417   0.04089158 -0.04100516  0.02145118 -0.03175703 -0.00395443\n",
      "  0.00930379 -0.04349047 -0.05442458 -0.04644591  0.02820535  0.04207855\n",
      "  0.02276783  0.048436    0.05210635  0.04672012  0.00951451  0.0703091\n",
      " -0.02640485  0.0475813  -0.01843026  0.00931365 -0.01305352 -0.05585784\n",
      "  0.0660333   0.03268431  0.03468495 -0.03586391  0.01381461  0.02895879\n",
      "  0.03642162  0.02666534  0.08444791  0.08286282 -0.11242419 -0.00119837\n",
      "  0.08917841  0.01062122  0.06015144  0.00852896 -0.00139073  0.02928519\n",
      "  0.01849444 -0.01242107 -0.03123884  0.04860897 -0.0956165  -0.07012261\n",
      " -0.01592329 -0.03914123  0.02228214 -0.0328876  -0.06596383  0.03431309\n",
      " -0.00405365  0.03094163  0.02792123 -0.08773304  0.02822864 -0.01494778\n",
      "  0.0166737  -0.05231164  0.0449587   0.0238482  -0.02778257 -0.00330673\n",
      "  0.0233527  -0.016963  ]\n"
     ]
    }
   ],
   "source": [
    "#printing face embedding\n",
    "print(trainface_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1e4b5f0-7c62-4d7a-a666-a2d42dac6c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainface_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95959f7f-b143-40ce-841c-4017c2dd0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving trainface embedding in a file\n",
    "from numpy import savez_compressed\n",
    "np.savez_compressed(\"C:\\\\FaceRecognitionFiles\\\\puru_face_embedding.npz\",trainface_embedding,\"Puru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567ddaf6-2966-4137-b51a-5df0e741c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with other images\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "detector = MTCNN()\n",
    "from keras_facenet import FaceNet\n",
    "embedder = FaceNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22658c1-4ba1-4d67-8bba-6136977015c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load an image from filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4374b6f4-fe2c-4077-b790-3b10f2e7bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = cv2.imread(\"C:\\\\FaceRecognitionFiles\\\\trainImage.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "161ca8a4-384d-44cb-a3db-b4769a4ffdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('test image', faces)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b21ad772-9de0-41fd-87ac-c69f6f1bc566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load image from camera\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97edc594-3be5-4b7c-b772-685215061b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    has_image, image = cap.read()\n",
    "    if not has_image:\n",
    "        print('No frames are captured')\n",
    "    #frame = cv2.resize(frame,(160,160))\n",
    "    \n",
    "cv2.imshow('testimage',image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c89ee604-d10c-4b8d-bdb1-9a7e654c21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "861d971e-8660-45d0-9421-1d0b0f71bcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "2/2 [==============================] - 0s 256ms/step\n"
     ]
    }
   ],
   "source": [
    "# Loading the image in frame variable using opencv\n",
    "#frame = cv2.imread(\"C:\\\\FaceRecognitionFiles\\\\trainImage.jpg\")\n",
    "\n",
    "# Loading haar cascade classifier in classifier variable using opencv\n",
    "classifier = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Pre-processing the image, converting it to greyscale\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detecting faces from image using classifier (scalefactor[how much image is reduced at each scale], minimum no of neighbours[number of neighbours each candidate should have to retain it])\n",
    "faces = classifier.detectMultiScale(gray, 1.1, 6)\n",
    "#faces is a 2d array containing only xywh of all the people detected in the image\n",
    "#frame = cv2.imread(\"C:\\\\FaceRecognitionFiles\\\\trainImage.jpg\")\n",
    "#trainface_metadata = detector.detect_faces(frame) #extracting faces from trainImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "93e407a1-09e5-4aed-b695-580369927db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of faces =  0\n"
     ]
    }
   ],
   "source": [
    "print('number of faces = ',len(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e226d0d0-6b7e-4e55-b74e-2a5eb58acca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "585e66ae-1f02-45f3-a6eb-c16039739268",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = faces[0][0]\n",
    "y = faces[0][1]\n",
    "w = faces[0][2]\n",
    "h = faces[0][3]\n",
    "detected_face = cv2.rectangle(faces, (x,y),(x+w,y+h), color=255, thickness=3) #see the detected face outlines\n",
    "cv2.imshow('Detected face with rectangle',detected_face)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49953f4-9aab-4afd-9abc-cecece33f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of faces\n",
    "def get_faces(image, faces_metadata, required_size = (160,160)):\n",
    "    faces = []\n",
    "    for i in range(len(faces_metadata)):\n",
    "        face_index=0\n",
    "        x = int(faces_metadata[i]['box'][0])\n",
    "        y = int(faces_metadata[i]['box'][1])\n",
    "        w = int(faces_metadata[i]['box'][2])\n",
    "        h = int(faces_metadata[i]['box'][3])\n",
    "        \n",
    "        face = image[y:y+h, x:x+w]\n",
    "        face = Image.fromarray(face)\n",
    "        face = face.resize(required_size)\n",
    "        face = np.asarray(face)\n",
    "        faces.append(face)\n",
    "        \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fa3681a-522f-4c6d-a998-10f3a200988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "facesdata = get_faces(faces, faces_metadata, required_size = (160,160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7830a31-3cfe-4169-bd49-73f88e6b8249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of faces =  3\n"
     ]
    }
   ],
   "source": [
    "print('number of faces = ', len(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae2abb01-3dd7-4d9c-8503-2d2845e54938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embedding data of all faces\n",
    "def get_embedding_facenet(face_pixels):\n",
    "    face_pixels = face_pixels.astype('float32')\n",
    "    mean, std = face_pixels.mean(), face_pixels.std()\n",
    "    face_pixels = (face_pixels - mean)/std\n",
    "    #transform face into one smaple \n",
    "    samples = np.expand_dims(face_pixels, axis=0)\n",
    "    #make prediction to get embedding\n",
    "    embedding = embedder.model.predict(samples)\n",
    "    return embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "940a9747-6861-4b35-98a4-b15b10a4b086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "face_embeddings = []\n",
    "for i in range(len(facesdata)):\n",
    "    face_embedding = get_embedding_facenet(facesdata[i])\n",
    "    face_embeddings.append(face_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3592246c-151e-4a82-9dc6-9d727eb96cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading my embedding\n",
    "def load_numpy_faces(faces_dataset_numpy):\n",
    "    from numpy import load\n",
    "    data = load(faces_dataset_numpy)\n",
    "    faces, labels = data['arr_0'], data['arr_1']\n",
    "    return faces, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "219feadd-bdcf-435a-a178-40e542469fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_dataset_numpy = 'C:\\\\FaceRecognitionFiles\\\\puru_face_embedding.npz'\n",
    "puru_face_embedding, puru_name = load_numpy_faces(faces_dataset_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bb09656-e51a-483d-b583-3e3d2d003f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_distances = np.linalg.norm(face_embeddings - puru_face_embedding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9fd8ec4-6e00-40bf-889c-1c9c96e4c191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7161484, 1.2717508, 1.1657768], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "599679fb-30e4-4cf6-a38e-59f26812a34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face with closest distance is:  0\n"
     ]
    }
   ],
   "source": [
    "print('face with closest distance is: ', np.argmin(faces_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "162f94d9-f55b-443c-ad76-fb1afab1da46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 46  54  61]\n",
      "  [ 54  62  69]\n",
      "  [ 58  66  73]\n",
      "  ...\n",
      "  [ 85 100 109]\n",
      "  [ 79  94 103]\n",
      "  [ 83  99 111]]\n",
      "\n",
      " [[ 46  54  61]\n",
      "  [ 52  60  67]\n",
      "  [ 58  66  73]\n",
      "  ...\n",
      "  [ 84  99 108]\n",
      "  [ 78  93 102]\n",
      "  [ 80  96 108]]\n",
      "\n",
      " [[ 44  52  59]\n",
      "  [ 48  56  63]\n",
      "  [ 54  62  69]\n",
      "  ...\n",
      "  [ 87 102 111]\n",
      "  [ 81  96 105]\n",
      "  [ 82  98 110]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 33  39  38]\n",
      "  [ 35  41  40]\n",
      "  [ 36  42  41]\n",
      "  ...\n",
      "  [185 180 171]\n",
      "  [185 180 171]\n",
      "  [189 182 173]]\n",
      "\n",
      " [[ 33  39  38]\n",
      "  [ 33  39  38]\n",
      "  [ 34  40  39]\n",
      "  ...\n",
      "  [183 178 169]\n",
      "  [182 177 168]\n",
      "  [189 182 173]]\n",
      "\n",
      " [[ 33  39  38]\n",
      "  [ 32  38  37]\n",
      "  [ 32  38  37]\n",
      "  ...\n",
      "  [184 179 170]\n",
      "  [183 178 169]\n",
      "  [189 182 173]]]\n"
     ]
    }
   ],
   "source": [
    "print(facesdata[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "526f05ae-c041-4a72-8b29-35871cff6205",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(faces_distances[np.argmin(faces_distances)] < 1):\n",
    "    cv2.imshow('Answer', facesdata[np.argmin(faces_distances)])\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print('Face not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d218a4-faf1-42b5-bd1a-8f36e66cfbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
